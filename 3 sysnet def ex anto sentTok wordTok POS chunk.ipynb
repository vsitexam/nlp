{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9709a808",
   "metadata": {},
   "source": [
    "# 3A Study of wordnet using synsets, definitions, examples, synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b6c12ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('small.n.01'), Synset('small.n.02'), Synset('small.a.01'), Synset('minor.s.10'), Synset('little.s.03'), Synset('small.s.04'), Synset('humble.s.01'), Synset('little.s.07'), Synset('little.s.05'), Synset('small.s.08'), Synset('modest.s.02'), Synset('belittled.s.01'), Synset('small.r.01')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "print(wordnet.synsets(\"small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18bb651b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the slender part of the back\n",
      "Examples: []\n",
      "[Lemma('sell.v.01.sell')]\n"
     ]
    }
   ],
   "source": [
    "# defination and example of the word 'computer'\n",
    "print(wordnet.synset(\"small.n.01\").definition())\n",
    "\n",
    "# examples\n",
    "print(\"Examples:\",wordnet.synset(\"small.n.01\").examples())\n",
    "\n",
    "# get Antonyms\n",
    "print(wordnet.lemma('buy.v.01.buy').antonyms())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1923483d",
   "metadata": {},
   "source": [
    "# 3B Lemma Hyponym Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f3f082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
      "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "print(wordnet.synsets(\"computer\"))\n",
    "\n",
    "print(wordnet.synset(\"computer.n.01\").lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60df99d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('computer.n.01') --> ['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
      "Synset('calculator.n.01') --> ['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n"
     ]
    }
   ],
   "source": [
    "# all lemmas for each synset.\n",
    "for e in wordnet.synsets(\"computer\"):\n",
    "  print(f'{e} --> {e.lemma_names()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "987703a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')]\n"
     ]
    }
   ],
   "source": [
    "# print all lemmas for a given synset.\n",
    "print(wordnet.synset('computer.n.01').lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ddbb1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('computer.n.01')\n"
     ]
    }
   ],
   "source": [
    "# get the synset corresponding to lemma\n",
    "print(wordnet.lemma('computer.n.01.computing_device').synset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34bd6a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing_device\n"
     ]
    }
   ],
   "source": [
    "# Get the name of the lemma.\n",
    "print(wordnet.lemma('computer.n.01.computing_device').name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cff1c269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _WordNetObject.hyponyms of Synset('computer.n.01')>\n",
      "['analog_computer', 'analogue_computer', 'digital_computer', 'home_computer', 'node', 'client', 'guest', 'number_cruncher', 'pari-mutuel_machine', 'totalizer', 'totaliser', 'totalizator', 'totalisator', 'predictor', 'server', 'host', 'Turing_machine', 'web_site', 'website', 'internet_site', 'site']\n"
     ]
    }
   ],
   "source": [
    "# the list of hyponyms words of the computer\n",
    "syn = wordnet.synset('computer.n.01')\n",
    "print(syn.hyponyms)\n",
    "print([lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b98f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# the semantic similarity in wordnet.\n",
    "vehicle = wordnet.synset('vehicle.n.01')\n",
    "car = wordnet.synset('car.n.01')\n",
    "print(car.lowest_common_hypernyms(vehicle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8bca3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernyms of car:\n",
      "Synset('motor_vehicle.n.01')\n",
      "Synset('wheeled_vehicle.n.01')\n",
      "Synset('ride.v.02')\n"
     ]
    }
   ],
   "source": [
    "#hypernyms of a word\n",
    "synsets = wordnet.synsets('bike')\n",
    "\n",
    "# Find hypernyms for each synset\n",
    "hypernyms = []\n",
    "for synset in synsets:\n",
    "    for hypernym in synset.hypernyms():\n",
    "        hypernyms.append(hypernym)\n",
    "\n",
    "# Print the hypernyms\n",
    "print(\"Hypernyms of\", word + \":\")\n",
    "for hypernym in hypernyms:\n",
    "    print(hypernym)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac509e",
   "metadata": {},
   "source": [
    "# 3C Sentence Tok, Word Tok, POS Tag, Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a50c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk import tag\n",
    "from nltk import chunk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72d940f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization : \n",
      " ['Hello, I am Shlok.', 'I am learning Natural Language Processing.', 'Today is my NLP Exam.']\n"
     ]
    }
   ],
   "source": [
    "#sent tok\n",
    "para = \"Hello, I am Shlok. I am learning Natural Language Processing. Today is my NLP Exam.\"\n",
    "sents = tokenize.sent_tokenize(para)\n",
    "print(\"Sentence Tokenization : \\n\",sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c7a37a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenization : \n",
      "\n",
      "['Hello', ',', 'I', 'am', 'Shlok', '.']\n",
      "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.']\n",
      "['Today', 'is', 'my', 'NLP', 'Exam', '.']\n"
     ]
    }
   ],
   "source": [
    "#word tok\n",
    "print(\"Word tokenization : \\n\") \n",
    "for index in range(len(sents)): \n",
    "  words = tokenize.word_tokenize(sents[index])\n",
    "  print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38a74e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'am', 'Shlok', '.']\n",
      "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.']\n",
      "['Today', 'is', 'my', 'NLP', 'Exam', '.']\n",
      "\n",
      "POS Tagging :  \n",
      " [[('Hello', 'NNP'), (',', ','), ('I', 'PRP'), ('am', 'VBP'), ('Shlok', 'NNP'), ('.', '.')], [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('.', '.')], [('Today', 'NN'), ('is', 'VBZ'), ('my', 'PRP$'), ('NLP', 'JJ'), ('Exam', 'NNP'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging \n",
    "tagged_words = [] \n",
    "\n",
    "for index in range(len(sents)): \n",
    "    words = tokenize.word_tokenize(sents[index])\n",
    "    print(words)\n",
    "    tagged_words.append(tag.pos_tag(words)) \n",
    "    \n",
    "print(\"\\nPOS Tagging :  \\n\",tagged_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c334899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking : \n",
      "\n",
      "[Tree('S', [Tree('GPE', [('Hello', 'NNP')]), (',', ','), ('I', 'PRP'), ('am', 'VBP'), Tree('PERSON', [('Shlok', 'NNP')]), ('.', '.')]), Tree('S', [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), Tree('ORGANIZATION', [('Natural', 'NNP'), ('Language', 'NNP')]), ('Processing', 'NNP'), ('.', '.')]), Tree('S', [('Today', 'NN'), ('is', 'VBZ'), ('my', 'PRP$'), Tree('ORGANIZATION', [('NLP', 'JJ'), ('Exam', 'NNP')]), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "# chunking \n",
    "tree = [] \n",
    "for index in range(len(sents)): \n",
    "    tree.append(chunk.ne_chunk(tagged_words[index])) \n",
    "    \n",
    "print(\"chunking : \\n\") \n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb664d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
